---
title: "Trump Tweets - NLP"
author: "Sakthi Kripa Selvan"
date: "12/10/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
```

# Setup

```{r libraries}
library(ggplot2)
library(dplyr)
library(readr)
library(tidytext)
library(stringr)
library(lubridate)
library(gridExtra)
library(grid)
library(glmnet)
```

# Part A

## Problem 1

Loading the dataset. While loading we load the id column as character type. 

```{r readcsv}
donald_tweets <- read_csv(file = paste(
    "C:\\Users\\sakthikripa\\Desktop\\IDMP - Kylie\\hw6\\",
    "twitter\\realDonaldTrump-20201106.csv",
    sep=""),
                          col_types = cols(id=col_character()))
```

Structure the tweets in the dataset and processing the data as asked:\\

* Do not include re-tweets\\

* Do not include tweets without any spaces\\

* Remove stop words and “&amp”\\

* Remove variations on Donald Trump’s name\\

* Remove URLs and twitter @usernames\\

```{r}
remove_reg <- "https?://[^\\s]+|&amp;|&lt;|&gt;|\bRT\\b|@\\w+ *"

tidy_tweets <- donald_tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_remove_all(text, remove_reg)) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!str_detect(word,'[:space:]')) %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))
tidy_tweets
```

Changing the format of date and extracting year for future problems. 

```{r}
tidy_tweets$years = year(as.Date(tidy_tweets$date))
tidy_tweets
```

Removing variations of Donald Trump and filtering for the top 20 words in the dataset

```{r}
custom_stop_words <- tibble(word = c("trump","Trump","Donald","donald","DonaldTrump"
                                     ,"amp","donaldtrump","realdonaldtrump"))

tidy_tweets2 <- tidy_tweets %>%
  anti_join(custom_stop_words, by = "word")

words_count <- tidy_tweets2 %>%
  group_by(word) %>% 
  count() %>% 
  arrange(-n)
words_count<-head(words_count,20)
words_count

```

Plotting a bar graph for the top 20 common words in Donald Trump's tweets

```{r}
ggplot(words_count,aes(x=reorder(word, n), y=n)) + 
  geom_bar(stat='identity',fill = "#FF6666")+
  coord_flip()+
  labs(title = "Top 20 most common terms in Donald Trump’s tweets", y = "Count",x="Words")
```

the tweets were structured into tidy format using the toke = "tweets" and processed as required. Further it is observed the most common word in Donald Trump's tweets was 'President' followed by the word 'People'

## Problem 2

Plotting for top 20 words common words in Donald Trump's tweets for every year from 2015 to 2020

```{r}
words_year_df <- tidy_tweets2 %>%
  group_by(years,word) %>% 
  count()  %>% 
  filter(years >2014)%>%
  arrange(desc(years),desc(n)) %>%
  group_by(years) %>% 
  slice(1:20)
```

Plotting a bar grpah for each year

```{r, fig.width=6, fig.height=10}
p1 <- words_year_df %>% filter(years == 2015)
p2 <- ggplot(p1,aes(x=reorder(word, n), y=n)) + 
  geom_bar(stat='identity',fill = "#282968")+
  coord_flip()+
  labs(title = "2015", y = "Count",x="Words")

p3 <- words_year_df %>% filter(years == 2016)
p4 <- ggplot(p3,aes(x=reorder(word, n), y=n)) + 
  geom_bar(stat='identity', fill = "#f7a721")+
  coord_flip()+
  labs(title = "2016", y = "Count",x="Words")

p5 <- words_year_df %>% filter(years == 2017)
p6 <- ggplot(p5,aes(x=reorder(word, n), y=n)) + 
  geom_bar(stat='identity', fill = "#3A225D")+
  coord_flip()+
  labs(title = "2017", y = "Count",x="Words")

p7 <- words_year_df %>% filter(years == 2018)
p8 <- ggplot(p7,aes(x=reorder(word, n), y=n)) + 
  geom_bar(stat='identity', fill = "#00274C")+
  coord_flip()+
  labs(title = "2018", y = "Count",x="Words")

p9 <- words_year_df %>% filter(years == 2019)
p10 <- ggplot(p9,aes(x=reorder(word, n), y=n)) + 
  geom_bar(stat='identity', fill = "#CBA92B")+
  coord_flip()+
  labs(title = "2019", y = "Count",x="Words")

p11 <- words_year_df %>% filter(years == 2020)
p12 <- ggplot(p11,aes(x=reorder(word, n), y=n)) + 
  geom_bar(stat='identity', fill = "#f7a721")+
  coord_flip()+
  labs(title = "2020", y = "Count",x="Words")

grid.arrange(p2,p4,p6,p8,p10,p12, nrow = 3, ncol = 2,
 top = textGrob("Top 20 most common terms in Donald Trump’s tweets for years 2015 to 2020",
                           gp=gpar(fontsize=14,font=3)))
```


## Problem 3

Treating year as a “document” to calculate the tf-idf for each term and year. 

```{r}
tidy_tweets3 <- tidy_tweets2 %>% filter(years > 2014)
tweets_tf_period <- tidy_tweets3 %>%
  count(years, word, sort=TRUE) %>%
  bind_tf_idf(term=word, document=years, n=n)
```

Visualize the top 20 most characteristic terms in Donald Trump’s tweets for each year from 2015-2020, and comment on the visualization.

```{r, fig.width=6, fig.height=10}
tweets_tf_period %>%
  filter(str_detect(word, "[:alpha:]")) %>%
  group_by(years) %>%
  top_n(20, wt=tf_idf) %>%
  ggplot(aes(x=reorder_within(word, tf_idf, years), y=tf_idf, fill=years)) +
  geom_col(show.legend=FALSE) +
  facet_wrap(~years, scales="free",ncol = 2) +
  coord_flip() +
  labs(x="Word", y="tf-idf",
       title="Most defining words (by year)",
       fill="Years") +
  scale_x_reordered() +
  scale_y_continuous(labels=NULL) +
  theme_minimal()
```

# Part B

## Problem 4

Filtering data to include tweets only from 2016 to 2020. And fitting sparse regression models to predict the number of retweets that a tweet will get.

```{r}
tweet_model_data <- tidy_tweets2 %>%
  filter(years >= 2016) %>%
  count(id, word) %>%
  cast_sparse(id,word,n)

tweet_model_rows <- tibble(id = rownames(tweet_model_data))

tweets_model_full <- left_join(tweet_model_rows,donald_tweets, by = "id")
```

Fitting and plotting the model

```{r}
model_fit <- glmnet(tweet_model_data,tweets_model_full$retweets )
plot(model_fit, xvar = "lambda", label = TRUE)
```

Using cross validation to select the lambda paramter

```{r}
model_fit_cv <- cv.glmnet(tweet_model_data,tweets_model_full$retweets)
plot(model_fit_cv)
```



```{r}
print(model_fit_cv)
```

Putting the lambda values into a variable for future use. 

```{r}
c2 <- coef(model_fit_cv, s = "lambda.1se")
c1 <- coef(model_fit_cv, s = "lambda.min")
```

## Problem 5

Extracting the coefficients from the best model

```{r}
coef_term <- as.data.frame(as.matrix(c2)) %>%
  rename(coef = 1) %>%
  filter(coef != 0) %>%
  top_n(15)

p1 <- arrange(coef_term, desc(coef))
head(p1,10)
```

Plotting bar graph to visulatize the relationship of coefficients with retweets 

```{r}
df <- cbind(coeef_term = rownames(coef_term), coef_term)
rownames(coef_term) <- 1:nrow(coef_term)
ggplot(df,aes(x=reorder(coeef_term, coef), y=coef)) + 
  geom_bar(stat='identity', fill = "#f7a721")+
  coord_flip()+
  labs(title = "Relationship of coefficients with retweets", y = "Coef",x="Coef_term")
```

Also looking at the relationship of coefficients with retweets when lambda which gives least mse is taken

```{r}
coef_term <- as.data.frame(as.matrix(c1)) %>%
  rename(coef = 1) %>%
  filter(coef != 0) %>% 
  top_n(10)

p2 <- arrange(coef_term, desc(coef))
head(p2,10)
```

